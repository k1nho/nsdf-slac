{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute in a shell:\n",
    "\n",
    "```\n",
    "mkdir supercdms-data\n",
    "alias ='aws s3 --profile slac_public --endpoint-url https://maritime.sealstorage.io/api/v0/s3 --no-verify-ssl'\n",
    "s3 ls --recursive s3://utah/supercdms-data/CDMS/UMN/R68/Raw/  | awk '{print $4}' > supercdms-data/list.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 39522 .mid.gz files\n",
      "Done supercdms-data/CDMS/UMN/R68/Raw/07180808_1558/07180808_1558_F0001.mid.gz 684,760\n"
     ]
    }
   ],
   "source": [
    "import os,sys,boto3,json,xmltodict,itertools,subprocess\n",
    "from botocore.client import Config\n",
    "import midas.file_reader\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# /////////////////////////////////////////\n",
    "def GetBanksData(filename):\n",
    "    reader = midas.file_reader.MidasFile(filename)\n",
    "    for E,evt in enumerate(reader):\n",
    "      for bank_name, bank in evt.banks.items():\n",
    "        yield (evt, bank)\n",
    "\n",
    "# ///////////////////////////////////////////////////////////////////\n",
    "def LoadXML(filename):\n",
    "\twith open(filename, 'rt') as file: \n",
    "\t\tbody = file.read() \n",
    "\treturn \n",
    "\n",
    "# ///////////////////////////////////////////////////////////////////\n",
    "def Merge(d1, d2):\n",
    "    return {**d1, **d2} if d1 and d2 else (d1 or d2)\n",
    "\n",
    "# ///////////////////////////////////////////////////////////////////\n",
    "def Concat(v):\n",
    "    return np.array(itertools.chain.from_iterable(v))\n",
    "\n",
    "# ///////////////////////////////////////////////////////////////////\n",
    "def PreviewBinary(obj):\n",
    "\n",
    "    if obj is None or isinstance(obj, str) :\n",
    "        return {\"type\": type(obj).__name__, \"value\": obj}\n",
    "\n",
    "    if isinstance(obj, bytes):\n",
    "        return {\n",
    "            \"type\": type(obj).__name__,\n",
    "            \"len\": len(obj),\n",
    "        }\n",
    "\n",
    "    assert(isinstance(obj,(tuple,list,np.ndarray)))\n",
    "    return {\n",
    "            \"type\": type(obj).__name__,\n",
    "            \"shape\": obj.shape if hasattr(obj, \"shape\") else [len(obj)],\n",
    "            \"dtype\": str(obj.dtype) if hasattr(obj, \"dtype\") else type(obj[0]).__name__,\n",
    "            \"vmin\": str(np.min(obj)),\n",
    "            \"vmax\": str(np.max(obj))\n",
    "        }\n",
    "\n",
    "    raise Exception(\"problem\")\n",
    "\n",
    "# /////////////////////////////////////////\n",
    "def ParseEvent(evt):\n",
    "\n",
    "    ret={}\n",
    "\n",
    "    ret[\"header\"]={\n",
    "        \"event_id\": evt.header.event_id,  # int\n",
    "        \"trigger_mask\": evt.header.trigger_mask,  # int\n",
    "        \"serial_number\":evt.header.serial_number,  # int\n",
    "        \"timestamp\": evt.header.timestamp,  # int UNIX timestamp of event\n",
    "        \"event_data_size_bytes\": evt.header.event_data_size_bytes,  # int Size of all banks\n",
    "    }\n",
    "    ret[\"all_bank_size_bytes\"]=evt.all_bank_size_bytes  # (int)\n",
    "    ret[\"flags\"]=evt.flags\n",
    "    ret[\"non_bank_data\"]=evt.non_bank_data # (bytes or None) - Content of some special events that don't\n",
    "    \n",
    "    ret[\"banks\"]={}\n",
    "    for bank_name, bank in evt.banks.items():\n",
    "        ret[\"banks\"][bank.name]={\n",
    "            \"name\": bank.name,  # (str) - 4 characters\n",
    "            \"type\": bank.type,  # (int) - See `TID_xxx` members in `midas` module\n",
    "            \"size_bytes\": bank.size_bytes,  # (int)\n",
    "            \"data\": bank.data # (tuple of int/float/byte etc, or a numpy array if use_numpy is specified when unpacking),\n",
    "        }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# ////////////////////////////////////////////////////////////////\n",
    "def Shell(cmd):\n",
    "\treturn subprocess.check_output(cmd, shell=True, text=True)\n",
    "\n",
    "# /////////////////////////////////////////\n",
    "def ConnectS3(bucket_name=None, endpoint_url=None,aws_access_key_id=None, aws_secret_access_key=None, signature_version = None):\n",
    "    config = Config(signature_version = signature_version)\n",
    "    s3 = boto3.resource('s3', endpoint_url=endpoint_url, aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, config=config,)\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    return bucket\n",
    "\n",
    "# /////////////////////////////////////////\n",
    "def ParseXML(body):\n",
    "    d=xmltodict.parse(body) \n",
    "    assert(isinstance(d,dict))\n",
    "    return d\n",
    "\n",
    "# /////////////////////////////////////////\n",
    "def SaveBinary(filename, data):\n",
    "    os.makedirs(os.path.dirname(filename),exist_ok=True)\n",
    "    np.savez_compressed(filename, data=data)\n",
    "\n",
    "# /////////////////////////////////////////\n",
    "def SaveJSON(filename,d):\n",
    "    os.makedirs(os.path.dirname(filename),exist_ok=True)\n",
    "    with open(json_filename,\"w\") as out:\n",
    "        out.write(d)\n",
    "\n",
    "\n",
    "# ///////////////////////////////////////////////////////////////\n",
    "bucket=ConnectS3(\"utah\", 'https://maritime.sealstorage.io/api/v0/s3','any','any','s3v4')\n",
    "max_files=1\n",
    "pulses={}\n",
    "    \n",
    "with open(\"supercdms-data/list.txt\",\"r\") as f:\n",
    "  files=[it.strip() for it in f.readlines() if it.strip().endswith(\".mid.gz\")]\n",
    "print(\"found\",len(files),\".mid.gz files\")\n",
    "        \n",
    "for I,key in enumerate(files):\n",
    "\n",
    "    if max_files and I>=max_files:\n",
    "        break\n",
    "            \n",
    "    key_noext=key.replace(\".mid.gz\",\"\")\n",
    "\n",
    "    # download file\n",
    "    if not os.path.isfile(key):\n",
    "        os.makedirs(os.path.dirname(key),exist_ok=True)\n",
    "        bucket.download_file(key,key)\n",
    "        print(f\"Downloaded file {key} {os.path.getsize(key)}\")\n",
    "    else:\n",
    "        # print(f\"File {key} already exists {os.path.getsize(key)}\")\n",
    "        pass\n",
    "\n",
    "    # generate json\n",
    "    if True or not os.path.isfile(key_noext+\".json\"):\n",
    "        reader = midas.file_reader.MidasFile(key)\n",
    "        events=[]\n",
    "        for E,evt in enumerate(reader):\n",
    "            parsed=ParseEvent(evt)\n",
    "        \n",
    "            # parse xml\n",
    "            non_bank_data=parsed[\"non_bank_data\"]\n",
    "            if non_bank_data:\n",
    "                try:\n",
    "                    # I am loosing some char this\n",
    "                    body=non_bank_data.decode(\"latin-1\")\n",
    "                    sub_key=f\"{key_noext}/events/{E:05d}/non_bank_data.txt\"\n",
    "                    os.makedirs(os.path.dirname(sub_key),exist_ok=True)\n",
    "                    with open(sub_key,\"w\") as out: out.write(body)\n",
    "                    parsed[\"non_bank_data\"]={\"key\":sub_key}\n",
    "                except:\n",
    "                    sub_key=f\"{key_noext}/events/{E:05d}/non_bank_data.npz\"\n",
    "                    SaveBinary(sub_key, non_bank_data)\n",
    "                    parsed[\"non_bank_data\"]={\"key\":sub_key, **PreviewBinary(non_bank_data)}\n",
    "            else:\n",
    "                del parsed[\"non_bank_data\"]\n",
    "\n",
    "            # streamable\n",
    "            for bank_name, bank in parsed[\"banks\"].items():\n",
    "                data=bank[\"data\"]\n",
    "                if data: \n",
    "                    data=np.array(data)\n",
    "                    sub_key=f\"{key_noext}/events/{E:05d}/banks/{bank_name}/data.npz\"\n",
    "                    SaveBinary(sub_key,data)\n",
    "                    bank[\"data\"]={\"key\":sub_key, **PreviewBinary(data)} # remove binary\n",
    "                else:\n",
    "                    del bank[\"data\"]\n",
    "                    \n",
    "            events.append(parsed)\n",
    "\n",
    "        # save json\n",
    "        d=json.dumps(events, sort_keys=False, indent=2)\n",
    "        json_filename=f\"{key_noext}.json\"\n",
    "        SaveJSON(json_filename,d)\n",
    "\n",
    "        # if you want compressed\n",
    "        uncompressed_size=os.path.getsize(json_filename)\n",
    "        #Shell(f\"gzip --keep --force {json_filename}\")\n",
    "        #compressed_size=os.path.getsize(json_filename + '.gz')\n",
    "        #print(\"JSON size size\", f\"{uncompressed_size:,}\",f\"{compressed_size:,}\")\n",
    "\n",
    "    print(\"Done\",key,f\"{uncompressed_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype int64 shape (35022570,) vmin 0 vmax 3221225852\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "pulses=[]\n",
    "for it in glob.glob(\"supercdms-data/**/data.npz\",recursive=True):\n",
    "  pulses.append(np.array(np.load(it)[\"data\"]))\n",
    "pulse=np.concatenate(pulses)\n",
    "vmin=np.min(pulse)\n",
    "vmax=np.max(pulse)\n",
    "print(\"dtype\",pulse.dtype,\"shape\", pulse.shape,\"vmin\",vmin,\"vmax\",vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data...\n",
      "write uncompressed data done logic_box 0 35022570\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import OpenVisus as ov\n",
    "idx_filename='/mnt/c/big/visus-datasets/signal1d_slac/visus.idx'\n",
    "\n",
    "N=pulse.shape[0]\n",
    "shutil.rmtree(os.path.splitext(os.path.dirname(idx_filename))[0], ignore_errors=True)\n",
    "db=ov.CreateIdx(\n",
    "  url=idx_filename, \n",
    "  dims=[N],\n",
    "  fields=[ov.Field('data',ov.DType.fromString(str(pulse.dtype)),'row_major')], \n",
    "  compression=\"raw\", \n",
    "  arco=f\"{4*1024*1024}\")\n",
    "assert(os.path.isfile(idx_filename))\n",
    "\n",
    "print(\"Writing data...\")\n",
    "logic_box=logic_box=ov.BoxNi(ov.PointNi([0]),ov.PointNi([N]))\n",
    "db.write(pulse,  logic_box=logic_box)\n",
    "print(\"write uncompressed data done\",\"logic_box\",logic_box.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compressing data\")\n",
    "db=ov.LoadDataset(idx_filename)\n",
    "db.compressDataset(\"zip\") \n",
    "print(\"compress dataset done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_box=db.getLogicBox()\n",
    "print(\"logic_box\",logic_box)\n",
    "print(\"db.getMaxResolution()\",db.getMaxResolution())\n",
    "resolution=12\n",
    "data=db.read(logic_box=logic_box, max_resolution=resolution)\n",
    "print(f\"IDX read done dtype={data.dtype} shape={data.shape} vmin={np.min(data)} vmax={np.max(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import bokeh.io \n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "p = bokeh.plotting.figure(title=\"Simple line example\", x_axis_label='x', y_axis_label='y')\n",
    "y=data\n",
    "x=list(range(len(y)))\n",
    "p.line(x, y, legend_label=\"Pulse\", line_width=1)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy blocks to S3. For example:\n",
    "\n",
    "\n",
    "```bash\n",
    "aws s3 sync --no-verify-ssl --endpoint-url https://maritime.sealstorage.io/api/v0/s3 --profile sealstorage --size-only  C:/big/visus-datasets/signal1d_slac/ s3://utah/visus-datasets/signal1d_slac/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With `max` filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_filename_max='/mnt/c/big/visus-datasets/signal1d_slac_max/visus.idx'\n",
    "shutil.rmtree(os.path.splitext(idx_filename_max)[0], ignore_errors=True)\n",
    "field=ov.Field('data',f\"int64[2]\",'row_major')\n",
    "N=pulse.shape[0]\n",
    "\n",
    "db=ov.CreateIdx(url=idx_filename_max, \n",
    "  dim=1, \n",
    "  dims=[N],\n",
    "  fields=[field], \n",
    "  compression=\"raw\",\n",
    "  filters=['max'],\n",
    "  arco=2*1024*1024\n",
    ")\n",
    "assert(os.path.isfile(idx_filename_max))\n",
    "\n",
    "print(f\"Source pulse dtype={pulse.dtype} shape={pulse.shape} vmin={np.min(pulse):,} vmax={np.max(pulse)}\")\n",
    "pulse_max=np.zeros((N, 2), dtype=pulse.dtype)\n",
    "pulse_max[:,0]=pulse[:]\n",
    "vmin,vmax=np.min(pulse_max[:,0]),np.max(pulse_max[:,0])\n",
    "print(f\"New pulse with extra channel shape={pulse_max.shape} dtype={pulse_max.dtype} vmin={vmin:,} vmax={vmax:,}\")\n",
    "\n",
    "print(\"Writing data...\")\n",
    "logic_box=ov.BoxNi(ov.PointNi([0]),ov.PointNi([N]))\n",
    "db.write(pulse_max, logic_box=logic_box)\n",
    "print(f\"written max pulse logic_box=[{logic_box.toString()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute filter (SLOW OPERATION even ~8 minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB=1024*1024*1024\n",
    "db.computeFilter(db.getField(), 1*GB)\n",
    "print(\"Filter computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compressing data...\")\n",
    "db.compressDataset(\"zip\") \n",
    "print(\"compress dataset done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show subsampled and max:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id =ov.LoadDataset('/mnt/c/big/visus-datasets/signal1d_slac/visus.idx'    )\n",
    "db_max=ov.LoadDataset('/mnt/c/big/visus-datasets/signal1d_slac_max/visus.idx')\n",
    "logic_box=db_id.getLogicBox()\n",
    "resolution=8\n",
    "data_sub=db_id .read(logic_box=logic_box, max_resolution=resolution)\n",
    "data_max=db_max.read(logic_box=logic_box, max_resolution=resolution)[:,0]\n",
    "print(f\"read id  dtype={data_sub.dtype } shape={data_sub.shape } vmin={np.min(data_sub) } vmax={np.max(data_sub )}\")\n",
    "print(f\"read max dtype={data_max.dtype} shape={data_max.shape} vmin={np.min(data_max)} vmax={np.max(data_max)}\")\n",
    "p = bokeh.plotting.figure(title=\"Simple line example\", x_axis_label='x', y_axis_label='y')\n",
    "p.line(list(range(len(data_sub))), data_sub, legend_label=\"sub\", line_width=1, color=\"blue\" );p.circle(list(range(len(data_sub))),data_sub, color=\"blue\" )\n",
    "p.line(list(range(len(data_max))), data_max, legend_label=\"max\", line_width=1, color=\"green\");p.circle(list(range(len(data_max))),data_max, color=\"green\")\n",
    "bokeh.io.show(p, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy blocks to S3. For example:\n",
    "\n",
    "\n",
    "```bash\n",
    "aws s3 sync --no-verify-ssl --endpoint-url https://maritime.sealstorage.io/api/v0/s3 --profile sealstorage --size-only  C:/big/visus-datasets/signal1d_slac_max/ s3://utah/visus-datasets/signal1d_slac_max/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
